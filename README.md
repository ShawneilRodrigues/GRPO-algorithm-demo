
# ğŸ“˜ Understanding Group Relative Policy Optimization(GRPO)

A guide to the theory and implementation of **GRPO** for aligning Large Language Models (LLMs).
![GRPO vs PPO](images/image1.png)
---

##  Overview
This repository provides a **technical explanation** of the Group-wise Policy Optimization (GRPO) algorithm, a method for aligning LLMs with human preferences or specific objectives. It explains the **core equations** and connects them to a **practical Python implementation**.

---

## ğŸ¯ 1. The Goal: LLM Alignment
The aim of GRPO is to fine-tune a base LLM (**reference policy** `Ï€_ref`) to create a new, improved model (**learned policy** `Ï€_Î¸`).

### ğŸ“Œ Formal Objective:
![GRPO Objective](images/image.png)

- **Ï€_Î¸(y|x)**: Probability of the fine-tuned model generating response `y` for prompt `x`.
- **Ï€_ref(y|x)**: Probability of the base model generating the same response.
- **r(x, y)**: Reward function scoring each response.
- **Î²**: KL-divergence coefficient (penalizes deviation).
- **KL(Ï€_Î¸ || Ï€_ref)**: Keeps the new model close to the reference.

âœ… **Goal:** Maximize reward while preventing catastrophic forgetting.

---

## ğŸ“š 2. GRPO Loss: Theory to Practice

### ğŸ”¹ Step 1: Generate a Group of Responses
For each prompt `x`, generate `K` responses:
```python
# K = group_size_k
generated_outputs = model.generate(
    prompt_tokens,
    max_new_tokens=256,
    num_return_sequences=group_size_k,  # Generate K responses
    do_sample=True,
)
````

### ğŸ”¹ Step 2: Calculate Rewards & Weights

Use a reward function and apply **Softmax** to compute normalized weights:

![Softmax Weights](images/softmax.png)

```python
# Get raw reward scores r_i
rewards = torch.tensor(
    combined_reward_function(prompts, completions, answers), 
    device=device
)

# Apply softmax to get weights
weights = F.softmax(rewards, dim=0)
```

### ğŸ”¹ Step 3: Weighted Log-Likelihood Loss

Update `Ï€_Î¸` to favor high-reward responses:

![GRPO Loss](images/weightloss.png)

```python
total_loss = 0
for i in range(group_size_k):
    log_probs = -F.cross_entropy(
        response_logits.view(-1, response_logits.size(-1)), 
        response_tokens_masked.view(-1)
    )
    total_loss += weights[i] * log_probs

final_loss = -total_loss
final_loss.backward()
optimizer.step()
```

---

## ğŸ”§ 3. Stabilizing Training

###  PPO-Style Clipping

![PPO Clipping](images/ppo_clipping.png)

Prevents large, destabilizing updates.

###  KL Penalty (Î²)

![KL Penalty](images/kl-penalty.png)

Keeps policy close to the reference to avoid reward hacking.

* **High Î²:** Safer, less deviation.
* **Low Î²:** More exploration, risk of forgetting base knowledge.

---

## ğŸ“‚ Repository Structure

```
ğŸ“¦ grpo-llm
 â”£ ğŸ“œ README.md
 â”£ ğŸ“œ grpo_training.py
 â”£ ğŸ“œ reward_function.py
 â”£ ğŸ“‚ images/
 â”ƒ â”£ grpo_objective.png
 â”ƒ â”£ softmax_weights.png
 â”ƒ â”£ grpo_loss.png
 â”ƒ â”£ ppo_clipping.png
 â”ƒ â”— kl_penalty.png
 â”£ ğŸ“‚ models/
 â”— ğŸ“‚ data/
```

---

## ğŸ“œ References

* **GRPO** â€“ [Group-Relative Policy Optimization](https://arxiv.org/abs/2502.01652).
* **PPO:** [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347)
* **DPO:** [Direct Preference Optimization](https://arxiv.org/abs/2305.18290)

---

## ğŸ¤ Contributing

Contributions, pull requests, and discussions are welcome! 

---

