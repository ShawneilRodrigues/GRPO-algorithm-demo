
# ğŸ“˜ Understanding Group-wise Policy Optimization (GRPO)

A guide to the theory and implementation of **GRPO** for aligning Large Language Models (LLMs).

---

## ğŸš€ Overview
This repository provides a **technical explanation** of the Group-wise Policy Optimization (GRPO) algorithm, a method for aligning LLMs with human preferences or specific objectives. It explains the **core equations** and connects them to a **practical Python implementation**.

---

## ğŸ¯ 1. The Goal: LLM Alignment
The aim of GRPO is to fine-tune a base LLM (**reference policy** `Ï€_ref`) to create a new, improved model (**learned policy** `Ï€_Î¸`).

### ğŸ“Œ Formal Objective:
\[
The formal objective that GRPO aims to maximize is:

L 
GRPO
â€‹
 (Ï€ 
Î¸
â€‹
 )=E 
xâˆ¼D,yâˆ¼Ï€ 
Î¸
â€‹
 (â‹…âˆ£x)
â€‹
 [r(x,y)]âˆ’Î²â‹…E 
xâˆ¼D
â€‹
 [KL(Ï€ 
Î¸
â€‹
 (â‹…âˆ£x)âˆ£âˆ£Ï€ 
ref
â€‹
 (â‹…âˆ£x))]
Let's break down the components:
\]

- **Ï€_Î¸(y|x)**: Probability of the fine-tuned model generating response `y` for prompt `x`.
- **Ï€_ref(y|x)**: Probability of the base model generating the same response.
- **r(x, y)**: Reward function scoring each response.
- **Î²**: KL-divergence coefficient (penalizes deviation).
- **KL(Ï€_Î¸ || Ï€_ref)**: Keeps the new model close to the reference.

âœ… **Goal:** Maximize reward while preventing catastrophic forgetting.

---

## ğŸ“š 2. GRPO Loss: Theory to Practice

### ğŸ”¹ Step 1: Generate a Group of Responses
For each prompt `x`, generate `K` responses:
```python
# K = group_size_k
generated_outputs = model.generate(
    prompt_tokens,
    max_new_tokens=256,
    num_return_sequences=group_size_k,  # Generate K responses
    do_sample=True,
)
````

### ğŸ”¹ Step 2: Calculate Rewards & Weights

Use a reward function and apply **Softmax** to compute normalized weights:

```python
# Get raw reward scores r_i
rewards = torch.tensor(
    combined_reward_function(prompts, completions, answers), 
    device=device
)

# Apply softmax to get weights
weights = F.softmax(rewards, dim=0)
```

### ğŸ”¹ Step 3: Weighted Log-Likelihood Loss

Update `Ï€_Î¸` to favor high-reward responses:

```python
total_loss = 0
for i in range(group_size_k):
    log_probs = -F.cross_entropy(
        response_logits.view(-1, response_logits.size(-1)), 
        response_tokens_masked.view(-1)
    )
    total_loss += weights[i] * log_probs

final_loss = -total_loss
final_loss.backward()
optimizer.step()
```

---

## ğŸ”§ 3. Stabilizing Training

### âœ… PPO-Style Clipping

Prevents large, destabilizing updates:

$$
L_{clipped} = \min(ratio(Î¸)â‹…A_i, \; clip(ratio(Î¸), 1âˆ’Ïµ, 1+Ïµ)â‹…A_i)
$$

### âœ… KL Penalty (Î²)

Keeps policy close to the reference to avoid reward hacking:

$$
KL\_Penalty = Î²â‹…KL(Ï€_Î¸(â‹…âˆ£x)||Ï€_ref(â‹…âˆ£x))
$$

* **High Î²:** Safer, less deviation.
* **Low Î²:** More exploration, risk of forgetting base knowledge.

---

## ğŸ“‚ Repository Structure

```
ğŸ“¦ grpo-llm
 â”£ ğŸ“œ README.md
 â”£ ğŸ“œ grpo_training.py
 â”£ ğŸ“œ reward_function.py
 â”£ ğŸ“‚ models/
 â”— ğŸ“‚ data/
```

---

## ğŸ“œ References

* **Group-wise Policy Optimization (GRPO)** â€“ Inspired by group-wise preference optimization methods.
* **PPO:** [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347)
* **DPO:** [Direct Preference Optimization](https://arxiv.org/abs/2305.18290)

---

## ğŸ¤ Contributing

Contributions, pull requests, and discussions are welcome! ğŸ‰

---


